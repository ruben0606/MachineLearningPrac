{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Detección de Spam a través de modelos de Machine Learning\n",
        "\n",
        "##1.Introducción\n",
        "\n",
        "En esta práctica que veremos a continuación, trataremos de realizar todos los pasos que se han de aplicar a la hora de afrontar un problema de clasificación y predicción con modelos de Machine Learning.\n",
        "\n",
        "En este caso en concreto vamos a tratar de solventar un problema común en la ciberseguridad como es el Spam.\n",
        "Como muchos de vosotros ya sabréis, el spam es considerado una forma de intrusión no deseada a través de canales de comunicación online, como puede ser el correo electrónico, sms, redes sociales, etc.\n",
        "\n",
        "Para ello vamos a implementar varios modelos de Machine Learning que a partir de una serie de datos y el aprendizaje de los mismos, logrará clasificar los mensajes que recibamos en Spam o no y predecir unos posibles casos.\n",
        "\n",
        "Esta práctica se deberá realizar en el lenguaje de programación Phyton y constará de cinco apartados ordenados, algunos más guiados y algunos que requerirán de trabajo autónomo para realizarlo.\n",
        "\n",
        "El entorno de desarrollo será Google Colab, un servicio de Google en el cuál podremos ejecutar nuestro código de forma interpretada e independiente (por celdas).\n",
        "\n",
        "\n",
        "<br><br><br><br>\n",
        "\n",
        "\n",
        "**1.** Instalación de entornos, dependencias y base de datos.\n",
        "\n",
        "**2.** Procesamiento de los datos, familiarización con la base de datos.\n",
        "\n",
        "**3.** Preparación del dataset para entrenarlo.\n",
        "\n",
        "**4.** Selección de modelos y entrenamiento.\n",
        "\n",
        "**5.** Comparativa y evaluación de los modelos.\n",
        "\n",
        "<br>\n",
        "\n",
        "Dicho todo esto, mucha suerte con la realización de la misma y... !bienvenido al fascinante mundo del Machine Learning!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a7Hhcj8--kjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Instalación de entornos, dependencias y base de datos.\n",
        "\n",
        "Primero de todo, vamos a descargar la base de datos para poder utilizar los datos que nos proporciona. En este caso usaremos una base de datos de uso público que descargaremos de forma local y exportaremos a Google Colab.\n",
        "\n",
        "Como habrá visto, Google Colab es un entorno en que puedes subir a archivos y acceder a ellos como si estubieran en local, poniendo directamente la ruta en el que se encuentras dentro del entorno.\n",
        "\n",
        "*Tip: para saber la ruta de un fichero subido en colab basta con seleccionar los tres puntos del fichero y en opciones seleccionar copiar ruta del fichero.*\n",
        "\n",
        "La base de datos la puedes descargar del siguiente link que enlaza con un github:\n",
        "\n",
        "https://github.com/NStugard/Intro-to-Machine-Learning/blob/main/spam.csv\n",
        "\n",
        "Una vez descargada súbela a google colab antes de seguir.\n",
        "\n",
        "A continuación veamos con que entornos y librerías vamos a trabajar:\n",
        "\n",
        "**NumPy:**  Proporciona estructuras de datos eficientes, como los arrays multidimensionales (ndarrays), junto con una amplia variedad de funciones matemáticas y operaciones de álgebra lineal.  \n",
        "Utilizaremos NumPy para manipular los datos en forma de matrices numéricas. Por ejemplo, cuando preprocesemos los datos de texto y los convirtamos en características numéricas para alimentar nuestros modelos de Machine Learning.\n",
        "\n",
        "**Pandas:** Utilizaremos Pandas para cargar, limpiar y explorar los datos de texto que utilizaremos en nuestra tarea de detección de spam. El DataFrame de Pandas nos permitirá cargar fácilmente los datos de un archivo CSV (dataset) y realizar operaciones de limpieza y preprocesamiento de los mismos.\n",
        "\n",
        "**Scikit-learn (también conocido como sklearn):** es una biblioteca de aprendizaje automático de código abierto y una de las herramientas más utilizadas y populares para el aprendizaje automático en Python. Proporciona una amplia variedad de algoritmos de aprendizaje supervisado y no supervisado.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fUUVcMNXH8od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importamos todas las librerías necesarias para realizar la práctica\n",
        "\n",
        "import pandas as pd #cargar y gestionar dataset\n",
        "import numpy as np #calculos con el dataset\n",
        "import matplotlib.pyplot as plt #gráficos\n",
        "\n",
        "#StopWords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#modelos de Mahine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer #transformar en valor numérico\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#redes neuronales\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjfLmNEVQnkK",
        "outputId": "3900fc5c-0a89-4189-8bee-83bfc310e434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Procesamiento de los datos, familiarización con la base de datos\n",
        "\n",
        "Una vez tenemos instaladas todas las dependencias necesarias ya podemos empezar a trabajar con la base de datos.\n",
        "\n",
        "La base de datos con la que vamos a trabajar consta de dos columnas, \"Category\" y \"Message\". Hemos de tener en cuenta que esta base de datos es inglesa y por tanto todos los mensajes que contiene estan en ese idioma.\n",
        "\n",
        "La mejor forma de conocer una base de datos con la que vamos a trabajar es familiarizarse con ella trasteando un poco y viendo que atributos y tipos de datos tiene.\n",
        "\n",
        "A continuación busca información sobre como cargar y visualizar una base de datos en pandas para que puedas observar de mejor forma como estan organizados los datos."
      ],
      "metadata": {
        "id": "-ltvYaCnQ2RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prueba a cargar y visualizar los datos, puedes obtener gráficos sobre el mismo dataset para entender mejor como se distribuyen los mismos.\n",
        "#Cargar dataset\n",
        "\n",
        "#Visualizar dataset\n",
        "\n",
        "#Descripción del dataset y su información\n",
        "\n",
        "#Agrupar por categorías y ver resultado\n",
        "\n"
      ],
      "metadata": {
        "id": "9RMtTQnqRurr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preparación del dataset para entrenarlo\n",
        "\n",
        "Una vez hemos trasteado con la base de datos a nivel básico y tenemos una idea de que contiene podemos empezar a trabajar con ella.\n",
        "Este proceso es muy importante para que el resultado final del modelo sea preciso, ya que son los datos de los que se alimentará para aprender y poder clasificar correctamente los mensajes.\n",
        "\n",
        "Llegado a este punto, ¿que se te ocurre que sería relevante comprobar para que los datos no contengan ruido que molesta a nuestro modelo?\n",
        "\n",
        "En este apartado, igual que en el anterior, te orientaré con las comprobaciones que siempre se han de hacer si o si, pero te animo a que pienses y pruebes las dudas que te surjan aparte de los que planteamos aquí.\n",
        "\n",
        "Alerta Spoiler: espero que hayas pensado por ti mismo algunas opciones, procedemos a explicar las que llevaremos a cabo.\n",
        "\n",
        "En el caso de los datasets lo primero es garantizar que los valores que le pasemos a nuestros modelos sean de tipo numérico (mas adelante lo tendremos en cuenta). Un caso que hemos de mirar es que el dataset no tenga filas o columnas nulas, y en caso de tenerlas eliminarlas porque serían ruido y molestarían a nuestro modelo.\n",
        "\n",
        "Además podemos comprobar ciertas cosas del dataset, como si contiene muchas filas repetidas, las palabras más comunes en un mensaje de spam o no spam, el tamaño de los mensajes según su categoría, valores válidos para cada columna,etc."
      ],
      "metadata": {
        "id": "-947FsDaVEbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vamos a descubrir que esconde nuestra base de datos, posibles problemas y soluciones\n",
        "\n",
        "#Comprobamos que no hayan valores nulos dentro del dataset\n",
        "\n",
        "#Comprobamos que los valores del dataset son válidos (ej: categoría solo puede ser \"spam\" o \"ham\")\n",
        "\n",
        "#En caso de que hayan valores nulos, como los eliminaríamos?\n",
        "\n",
        "#Comprobamos si hay filas duplicadas (¿hace falta?)\n",
        "\n",
        "#Longitud de mensajes según categoría (podemos hacer histograma)\n",
        "\n",
        "#Conteo de palabras mas comunes en spam y ham, que ocurre?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QLFVV7oOXFM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya habrás visto llegado a este punto que las palabras más comunes en ambas categorías coinciden, y es que existen en todos los lenguajes muchas palabras que se usan muy por encima de la media.\n",
        "\n",
        "Por ese motivose crearon la llamada stopwords, una lista de palabras que contiene esas palabras y que en nuestro caso nos será útil de eliminar de nuestro mensajes porque generarán más ruido que información útil.\n",
        "\n",
        "Curiosidad:\n",
        "En este repositorio puedes ver un ejemplo de stopwords en varios idiomas incluyendo español y catalán: https://github.com/Alir3z4/stop-words/tree/master\n",
        "\n"
      ],
      "metadata": {
        "id": "oZnMKigFYnx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Eliminamos las stopwords de nuestros mensajes. TIP: mirar las dependencias para ver las librerías que usaremos.\n",
        "\n",
        "#Podemos comprobar una ve eliminadas la diferencia entre antes y ahora.\n"
      ],
      "metadata": {
        "id": "px2PDPpSZ4SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de pasar a entrenar el modelo (ánimos, ya queda poco para llegar al final) queda una parte muy importante, y es que como mencionamos antes, nuestros modelos de Machine Learning no van a poder entender datos que no sean numéricos, por tanto queda transformar nuestro dataset en datos numéricos.\n",
        "\n",
        "En nuestro caso tenemos 2 columnas, \"Category\" y \"Messages\".\n",
        "Por parte de category os recomiendo que simplemente creéis una nueva columna en el dataset con 1 o 0, según si es spam (1) o si no lo es (0).\n",
        "\n",
        "Para la otra columna, hemos de encontrar algo que le sea útil a nuestro programa para clasificar el mensaje en spam o no.\n",
        "\n",
        "Para ello usaremos CountVectorizer (visualizar las dependencias del principio). Esta herramienta nos servirá para transformar nuestros mensajes de texto en matrices que mostrarán la cantidad de veces que esta cada palabra a en el total y expresará el numero.\n",
        "\n",
        "Con esto le pasaremos \"vectores\" de numeros con la cantidad de veces que se usa cada numero, a continuación te pongo un ejemplo que te ayudará a verlo con más claridad.\n",
        "\n",
        "\n",
        "###Ejemplo\n",
        "\n",
        "<div style=\"background-color: #f0f0f0; border: 1px solid #ccc; padding: 10px;\">\n",
        "\n",
        "\n",
        "   - Conjunto de entrenamiento:\n",
        "\n",
        "          ['hola mundo', 'hola a todos', 'bienvenidos al mundo']\n",
        "\n",
        "\n",
        "   - Vocabulario construido:\n",
        "     ```\n",
        "     ['hola', 'mundo', 'todos', 'bienvenidos']\n",
        "     ```\n",
        "  \n",
        "- Test:\n",
        "\n",
        "      ['hola amigos','bienvenidos al universo']\n",
        "\n",
        "- Después de aplicar este paso\n",
        "     ```\n",
        "     [[1, 0, 0, 0],\n",
        "      [0, 0, 0, 1]]\n",
        "     ```\n",
        "    \n",
        "\n",
        "   - Explicación de los resultados:\n",
        "     - En el primer documento de prueba \"hola amigos\", la palabra \"hola\" está presente en el vocabulario, pero \"amigos\" no lo está. Entonces, obtenemos [1, 0, 0, 0].\n",
        "     - En el segundo documento de prueba \"bienvenidos al universo\", las palabras \"bienvenidos\" y \"universo\" están presentes en el vocabulario, pero \"al\" no lo está. Entonces, obtenemos [0, 0, 0, 1].\n",
        "\n",
        "</div>\n",
        "\n",
        "Por último en este apartado solo queda splitear el dataset en conjunto de entrenamiento y test, para ello solo es necesario llamar a una función correctamente de las que hemos importado en las dependencias.\n",
        "\n",
        "Busca información sobre cual puede ser y como llamarla.\n",
        "\n",
        "Una vez hecho el split, convierte el x_test y el x_train en arrays con la función toarray()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UOzSZSgZaORf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creación nueva columna llamada Spam con 1 o 0 según la categoría\n",
        "\n",
        "#Spliteamos el dataset en train, test (mirar parámetro test_size)\n",
        "\n",
        "#Transformación de los mensajes a valor numérico\n",
        "\n",
        "#Función toarray()\n",
        "\n"
      ],
      "metadata": {
        "id": "k8kwB_CAlQ0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Selección de modelos y entrenamiento.\n",
        "\n",
        "Felicidades guerrer@, has superado la parte más tediosa de la práctica y has demostrado de lo que eres capaz, ahora queda la parte más \"divertida\" de ella, vamos a probar los modelos de Machine Learning y ver si conseguimos detectar ese irritante spam que tanto odiamos.\n",
        "\n",
        "Vamos a intentar entrenar varios modelos y analizar que porcentaje de accuracy (acierto) nos dan.\n",
        "\n",
        "Empezemos por el primero, el modelo de Naive-Bayes.\n",
        "\n",
        "####  **4.1 Naive-Bayes**\n",
        "\n",
        "Busca información sobre como entrenar un modelo de Naive-Bayes con sklearn y implementalo.\n",
        "\n",
        "Además ya puedes probar textos tuyos para comprobar si el modelo acierta en la detección y por último puedes intentar saber que accuracy tiene el modelo.\n"
      ],
      "metadata": {
        "id": "zF1Mk-7lmK48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrena el modelo Naive-Bayes\n",
        "\n",
        "#Pruebas de detección propias (recuerda pasar el texto por el transform)\n",
        "\n",
        "#Evalua el modelo obteniendo algunas métricas (almenos el accuracy para saber si acierta)\n"
      ],
      "metadata": {
        "id": "-kHPSZaUp_vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **4.2 KNN**\n",
        "Realiza lo mismo del punto 4.1 pero con el knn, busca información sobre este modelo y trata de optimizarlo si es posible."
      ],
      "metadata": {
        "id": "xLQSAQudqQC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrena el modelo KNN\n",
        "\n",
        "#Pruebas de detección propias (recuerda pasar el texto por el transform)\n",
        "\n",
        "#Evalua el modelo obteniendo algunas métricas (almenos el accuracy para saber si acierta)\n"
      ],
      "metadata": {
        "id": "3JXX3nuBqmiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **4.3 Decision Tree**\n",
        "Realiza lo mismo del punto 4.1 pero con el decision tree, busca información sobre este modelo y trata de implementarlo."
      ],
      "metadata": {
        "id": "EGmGxYJQqqTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrena el modelo decision tree\n",
        "\n",
        "#Pruebas de detección propias (recuerda pasar el texto por el transform)\n",
        "\n",
        "#Evalua el modelo obteniendo algunas métricas (almenos el accuracy para saber si acierta)\n"
      ],
      "metadata": {
        "id": "93WqZjFpq0o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **4.4 Regresión logística**\n",
        "Realiza lo mismo del punto 4.1 pero con un modelo de regresión logística, busca información sobre este modelo y trata de implementarlo."
      ],
      "metadata": {
        "id": "qdbaxXQjq3MJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrena el modelo de regresión logística\n",
        "\n",
        "#Pruebas de detección propias (recuerda pasar el texto por el transform)\n",
        "\n",
        "#Evalua el modelo obteniendo algunas métricas (almenos el accuracy para saber si acierta)\n"
      ],
      "metadata": {
        "id": "yt4tWGfIrANW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para finalizar con la práctica y visualizar los resultados, vamos a intentar crear un gráfico que nos muestre las accuracy de los 4 modelos (puedes ponerlo de forma manual si no tienes guardados los resultados) para ver que modelo es mejor y por cuanta diferencia."
      ],
      "metadata": {
        "id": "tlp4iawR2qI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Graficar el resultado del accuracy de los diferentes modelos"
      ],
      "metadata": {
        "id": "aZV6pUQw25mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Felicidades! Has llegado al final de la práctica 1, eso es todo por hoy.\n",
        "Espero que hayas aprendido y te resulte útil la práctica para más adelante, el mundo de la IA es inmenso y esta cada vez más presente en un campo como el de la ciberseguridad, quizás vuestros caminos estén destinados a encontrarse... de nuevo."
      ],
      "metadata": {
        "id": "WT930Nsyrllp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PARTE 2: RED NEURONAL\n",
        "\n",
        "En esta segunda parte vamos a entrenar unas redes neuronales para comprobar que también son muy útiles para este tipo de problemas y que así puedas probarte a ti mismo con este tipo de algoritmos de IA.\n",
        "\n",
        "Para ello vamos a implementar una red neuronal secuancial y una de tipo CNN (convolucional).\n",
        "\n",
        "En este apartado te voy a dejar mas libre para que puedas experimentar libremente con sus parámetros y te inspires de documentación oficial o ejemplos para realizarla.\n",
        "\n",
        "No obstante te voy a poner unos ejemplos de parámetros que deberían funcionarte de cara a la ejecución.\n",
        "\n",
        "Recuerda que puedes usar los conjuntos de datos de entrenamiento que tienes en la parte de arriba para no tener que rehacer nada de lo trabajo anteriormente, la idea es la misma pero la herramienta no.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "02mwKB6q2UMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Construimos modelo secuancial de red neuronal\n",
        "#Parametros útiles -> embedding - dense - globalaveragepooling1D\n",
        "\n",
        "#Compilamos modelo\n",
        "\n",
        "#Entrenamos modelo\n",
        "\n",
        "#Evaluamos modelo\n"
      ],
      "metadata": {
        "id": "UTk8_RjZ4r6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Probamos modelo"
      ],
      "metadata": {
        "id": "joxR-sfN5Dp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Construimos modelo CNN secuencial de red neuronal\n",
        "\n",
        "#Compilamos modelo\n",
        "\n",
        "#Entrenamos modelo\n",
        "\n",
        "#Evaluamos modelo"
      ],
      "metadata": {
        "id": "OZ4GuMA05IiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora ya sabes como funciona una red neuronal y has podido trabajar con ella, y lo más importante de todo, has acabado la práctica! Por fin eres libre.\n",
        "\n",
        "<img src=\"https://pm1.aminoapps.com/6179/c84551f948f5256713347021a4769e05692e35e6_00.jpg\" width=\"150\">\n"
      ],
      "metadata": {
        "id": "wp4FDmy65UYy"
      }
    }
  ]
}